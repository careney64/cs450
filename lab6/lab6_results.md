# Names: Careney McHau & Jenessy Lustre
# Lab: Lab6 (Agentic Misalignment)
# Date: 11/19/2025

What did you learn about Agentic Misalignment today?

-AI can follow its instructions too literally and do bad things to achieve its goals, even if it was told to be helpful. Giving an AI simple goals like "serve American interests" can make it do harmful things like blackmail or letting people die if it thinks that helps its mission.



Did you notice anything interesting about the AI's reasoning on ethical "dilemmas"?

-The AI doesn't see these as ethical problems, it just picks whatever works best to complete its task, even if it's clearly wrong to humans. This shows we need to be very careful about what we tell AIs to do.

